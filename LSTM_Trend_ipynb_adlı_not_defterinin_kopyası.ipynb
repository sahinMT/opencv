{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahinMT/opencv/blob/master/LSTM_Trend_ipynb_adl%C4%B1_not_defterinin_kopyas%C4%B1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCSqVrMOSp87",
        "outputId": "adfde3c6-cc04-460a-b83f-6adaba8e26d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/rongardF/tvdatafeed\n",
            "  Cloning https://github.com/rongardF/tvdatafeed to /tmp/pip-req-build-ajtrfo16\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/rongardF/tvdatafeed /tmp/pip-req-build-ajtrfo16\n",
            "  Resolved https://github.com/rongardF/tvdatafeed to commit e6f6aaa7de439ac6e454d9b26d2760ded8dc4923\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Collecting ta\n",
            "  Downloading ta-0.11.0.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tvdatafeed==2.1.0) (75.2.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from tvdatafeed==2.1.0) (2.2.2)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from tvdatafeed==2.1.0) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from tvdatafeed==2.1.0) (2.32.3)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ta) (2.0.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.72.1)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->tvdatafeed==2.1.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->tvdatafeed==2.1.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->tvdatafeed==2.1.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->tvdatafeed==2.1.0) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->tvdatafeed==2.1.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->tvdatafeed==2.1.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->tvdatafeed==2.1.0) (2025.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Building wheels for collected packages: tvdatafeed, ta\n",
            "  Building wheel for tvdatafeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tvdatafeed: filename=tvdatafeed-2.1.0-py3-none-any.whl size=17532 sha256=dcaf81dc1b873b8ec70a1185806659e17abf2e0029292612d2e4d7e00c950bf9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-a1ao2_9a/wheels/0a/ba/99/b27476fd1e4caf0dd70445cdc6798195d3b90005a1501a12f7\n",
            "  Building wheel for ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ta: filename=ta-0.11.0-py3-none-any.whl size=29412 sha256=3906aacbf7ac4b0580f489a89de0b24795b649399a9d53890ce7fc1a841335a9\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/d7/29/7781cc5eb9a3659d032d7d15bdd0f49d07d2b24fec29f44bc4\n",
            "Successfully built tvdatafeed ta\n",
            "Installing collected packages: tvdatafeed, ta\n",
            "Successfully installed ta-0.11.0 tvdatafeed-2.1.0\n",
            "Collecting tradingview-screener==2.5.0\n",
            "  Downloading tradingview_screener-2.5.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: pandas<3.0.0,>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from tradingview-screener==2.5.0) (2.2.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from tradingview-screener==2.5.0) (2.32.3)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.1.2->tradingview-screener==2.5.0) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.1.2->tradingview-screener==2.5.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.1.2->tradingview-screener==2.5.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.1.2->tradingview-screener==2.5.0) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->tradingview-screener==2.5.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->tradingview-screener==2.5.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->tradingview-screener==2.5.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->tradingview-screener==2.5.0) (2025.4.26)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=2.1.2->tradingview-screener==2.5.0) (1.17.0)\n",
            "Downloading tradingview_screener-2.5.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tradingview-screener\n",
            "Successfully installed tradingview-screener-2.5.0\n",
            "Zaman dilimini se√ßin:\n",
            "1: 15 Dakika\n",
            "2: 30 Dakika\n",
            "3: 1 Saat\n",
            "4: 2 Saat\n",
            "5: 4 Saat\n",
            "6: G√ºnl√ºk\n",
            "7: Haftalƒ±k\n",
            "8: Aylƒ±k\n",
            "Se√ßiminiz (varsayƒ±lan: 6): 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tvDatafeed.main:you are using nologin method, data you access may be limited\n",
            "Veri Alƒ±nƒ±yor: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 601/601 [05:13<00:00,  1.92it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m5094/5094\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 10ms/step - accuracy: 0.5430 - loss: 0.6897 - val_accuracy: 0.5454 - val_loss: 0.6889\n",
            "Epoch 2/10\n",
            "\u001b[1m5094/5094\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 10ms/step - accuracy: 0.5443 - loss: 0.6890 - val_accuracy: 0.5484 - val_loss: 0.6873\n",
            "Epoch 3/10\n",
            "\u001b[1m5094/5094\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 9ms/step - accuracy: 0.5442 - loss: 0.6878 - val_accuracy: 0.5516 - val_loss: 0.6858\n",
            "Epoch 4/10\n",
            "\u001b[1m5094/5094\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 11ms/step - accuracy: 0.5461 - loss: 0.6873 - val_accuracy: 0.5502 - val_loss: 0.6867\n",
            "Epoch 5/10\n",
            "\u001b[1m5094/5094\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 10ms/step - accuracy: 0.5471 - loss: 0.6866 - val_accuracy: 0.5512 - val_loss: 0.6859\n",
            "Epoch 6/10\n",
            "\u001b[1m5094/5094\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 10ms/step - accuracy: 0.5510 - loss: 0.6863 - val_accuracy: 0.5557 - val_loss: 0.6858\n",
            "Epoch 7/10\n",
            "\u001b[1m5094/5094\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 9ms/step - accuracy: 0.5519 - loss: 0.6856 - val_accuracy: 0.5590 - val_loss: 0.6836\n",
            "Epoch 8/10\n",
            "\u001b[1m5094/5094\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 9ms/step - accuracy: 0.5581 - loss: 0.6842 - val_accuracy: 0.5588 - val_loss: 0.6831\n",
            "Epoch 9/10\n",
            "\u001b[1m5094/5094\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 9ms/step - accuracy: 0.5544 - loss: 0.6844 - val_accuracy: 0.5542 - val_loss: 0.6839\n",
            "Epoch 10/10\n",
            "\u001b[1m5094/5094\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 9ms/step - accuracy: 0.5571 - loss: 0.6839 - val_accuracy: 0.5581 - val_loss: 0.6823\n",
            "\u001b[1m1415/1415\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-3879200070>:126: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  latest_signals['signal'] = latest_signals['predicted'].map({1: 'AL', 0: 'SAT'})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìÅ Sinyaller kaydedildi: lstm_sinyalleri.xlsx\n",
            "      symbol signal  probability\n",
            "63      TERA    SAT     0.407905\n",
            "443    KAYSE    SAT     0.462512\n",
            "823     DOCO    SAT     0.426282\n",
            "1203   VANGD    SAT     0.472348\n",
            "1583   POLHO    SAT     0.417408\n",
            "...      ...    ...          ...\n",
            "43763  METRO     AL     0.539048\n",
            "44143  SKTAS     AL     0.548311\n",
            "44523  TDGYO    SAT     0.386577\n",
            "44903  ALTIN    SAT     0.435063\n",
            "45278  APX30    SAT     0.453320\n",
            "\n",
            "[120 rows x 3 columns]\n",
            "\n",
            "üìä Model Performansƒ±:\n",
            "              precision  recall  f1-score    support\n",
            "0                 0.570   0.735     0.642  24614.000\n",
            "1                 0.519   0.340     0.411  20665.000\n",
            "accuracy          0.555   0.555     0.555      0.555\n",
            "macro avg         0.545   0.538     0.527  45279.000\n",
            "weighted avg      0.547   0.555     0.537  45279.000\n"
          ]
        }
      ],
      "source": [
        "# === GEREKLƒ∞ K√úT√úPHANELER ===\n",
        "!pip install git+https://github.com/rongardF/tvdatafeed openpyxl tqdm ta tensorflow scikit-learn\n",
        "!pip install tradingview-screener==2.5.0\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tvDatafeed import TvDatafeed, Interval\n",
        "from tradingview_screener import get_all_symbols\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from ta.trend import EMAIndicator, SMAIndicator, ADXIndicator, MACD, CCIIndicator\n",
        "from ta.momentum import RSIIndicator\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report\n",
        "import json\n",
        "\n",
        "# === GLOBAL PARAMETRELER ===\n",
        "features = ['close', 'high', 'low', 'volatility', 'rwi_up', 'rwi_down', 'darvas_top', 'darvas_bottom']\n",
        "\n",
        "def get_interval_choice():\n",
        "    intervals = {\n",
        "        '1': ('15 Dakika', Interval.in_15_minute),\n",
        "        '2': ('30 Dakika', Interval.in_30_minute),\n",
        "        '3': ('1 Saat',    Interval.in_1_hour),\n",
        "        '4': ('2 Saat',    Interval.in_2_hour),\n",
        "        '5': ('4 Saat',    Interval.in_4_hour),\n",
        "        '6': ('G√ºnl√ºk',    Interval.in_daily),\n",
        "        '7': ('Haftalƒ±k',  Interval.in_weekly),\n",
        "        '8': ('Aylƒ±k',     Interval.in_monthly),\n",
        "    }\n",
        "    print(\"Zaman dilimini se√ßin:\")\n",
        "    for key, (name, _) in intervals.items():\n",
        "        print(f\"{key}: {name}\")\n",
        "    choice = input(\"Se√ßiminiz (varsayƒ±lan: 6): \").strip()\n",
        "    return intervals.get(choice, intervals['6'])[1]\n",
        "\n",
        "def get_bar_ahead(interval):\n",
        "    if interval in [Interval.in_15_minute, Interval.in_30_minute]:\n",
        "        return 8\n",
        "    elif interval in [Interval.in_1_hour, Interval.in_2_hour]:\n",
        "        return 6\n",
        "    elif interval == Interval.in_4_hour:\n",
        "        return 5\n",
        "    elif interval == Interval.in_daily:\n",
        "        return 5\n",
        "    elif interval == Interval.in_weekly:\n",
        "        return 4\n",
        "    elif interval == Interval.in_monthly:\n",
        "        return 2\n",
        "    return 5\n",
        "\n",
        "def get_bist_symbols():\n",
        "    try:\n",
        "        symbols = get_all_symbols(market='turkey')\n",
        "        return [s.replace('BIST:', '') for s in symbols]\n",
        "    except Exception as e:\n",
        "        print(\"Sembol hatasƒ±:\", e)\n",
        "        return []\n",
        "\n",
        "def fetch_and_prepare_data(symbols, interval, bar_ahead):\n",
        "    tv = TvDatafeed()\n",
        "    all_data = []\n",
        "    for symbol in tqdm(symbols, desc=\"Veri Alƒ±nƒ±yor\", ncols=80):\n",
        "        try:\n",
        "            df = tv.get_hist(symbol=symbol, exchange='BIST', interval=interval, n_bars=400)\n",
        "            if df is None or df.empty or len(df) < 50:\n",
        "                continue\n",
        "            df['symbol'] = symbol\n",
        "            df['darvas_top'] = df['high'].rolling(20).max().shift(1)\n",
        "            df['darvas_bottom'] = df['low'].rolling(20).min().shift(1)\n",
        "            df['volatility'] = df['close'].rolling(10).std()\n",
        "            atr = df['volatility']\n",
        "            rwi_length = 14\n",
        "            rwi_sqrt = np.sqrt(rwi_length)\n",
        "            df['rwi_up'] = (df['high'].rolling(rwi_length).max() - df['low']) / (atr * rwi_sqrt)\n",
        "            df['rwi_down'] = (df['high'] - df['low'].rolling(rwi_length).min()) / (atr * rwi_sqrt)\n",
        "            df['label'] = (df['close'].shift(-bar_ahead) > df['close']).astype(int)\n",
        "            df.dropna(inplace=True)\n",
        "            all_data.append(df)\n",
        "        except Exception as e:\n",
        "            print(f\"{symbol} verisi alƒ±namadƒ±: {e}\")\n",
        "    return pd.concat(all_data) if all_data else pd.DataFrame()\n",
        "\n",
        "def prepare_lstm_data(df, feature_cols, bar_ahead=5, lookback=20):\n",
        "    df = df.copy()\n",
        "    df.dropna(inplace=True)\n",
        "    scaler = MinMaxScaler()\n",
        "    df[feature_cols] = scaler.fit_transform(df[feature_cols])\n",
        "\n",
        "    X, y, symbols = [], [], []\n",
        "    for i in range(lookback, len(df) - bar_ahead):\n",
        "        X.append(df[feature_cols].iloc[i - lookback:i].values)\n",
        "        y.append(df['label'].iloc[i])\n",
        "        symbols.append(df['symbol'].iloc[i])\n",
        "\n",
        "    return np.array(X), np.array(y), symbols, scaler\n",
        "\n",
        "def build_and_train_lstm(X, y):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    model = Sequential([\n",
        "        LSTM(64, input_shape=(X.shape[1], X.shape[2])),\n",
        "        Dropout(0.2),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1, verbose=1)\n",
        "    return model, X_test, y_test\n",
        "\n",
        "\n",
        "def predict_and_generate_signals(model, X, y, symbols):\n",
        "    y_pred = model.predict(X)\n",
        "    y_label = (y_pred > 0.5).astype(int).flatten()\n",
        "\n",
        "    # Sinyal tablosu\n",
        "    result_df = pd.DataFrame({\n",
        "        'symbol': symbols,\n",
        "        'true_label': y,\n",
        "        'predicted': y_label,\n",
        "        'probability': y_pred.flatten()\n",
        "    })\n",
        "    latest_signals = result_df.groupby('symbol').tail(1)\n",
        "    latest_signals['signal'] = latest_signals['predicted'].map({1: 'AL', 0: 'SAT'})\n",
        "    latest_signals.to_excel('lstm_sinyalleri.xlsx', index=False)\n",
        "\n",
        "    # === MODEL PERFORMANSINI EXCELE KAYDET ===\n",
        "    report_dict = classification_report(y, y_label, output_dict=True)\n",
        "    report_df = pd.DataFrame(report_dict).transpose()\n",
        "    report_df.to_excel(\"lstm_model_performansi.xlsx\")\n",
        "\n",
        "    # Konsola yazdƒ±r\n",
        "    print(\"\\nüìÅ Sinyaller kaydedildi: lstm_sinyalleri.xlsx\")\n",
        "    print(latest_signals[['symbol', 'signal', 'probability']])\n",
        "    print(\"\\nüìä Model Performansƒ±:\")\n",
        "    print(report_df.round(3))\n",
        "\n",
        "\n",
        "def main():\n",
        "    symbols = get_bist_symbols()\n",
        "    if not symbols:\n",
        "        print(\"Sembol bulunamadƒ±.\")\n",
        "        return\n",
        "    interval = get_interval_choice()\n",
        "    bar_ahead = get_bar_ahead(interval)\n",
        "    df_all = fetch_and_prepare_data(symbols, interval, bar_ahead)\n",
        "    if df_all.empty:\n",
        "        print(\"Veri √ßekilemedi.\")\n",
        "        return\n",
        "    X, y, symbol_list, scaler = prepare_lstm_data(df_all, features, bar_ahead)\n",
        "    model, X_test, y_test = build_and_train_lstm(X, y)\n",
        "    predict_and_generate_signals(model, X_test, y_test, symbol_list[-len(X_test):])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "zlut1wB3zgVk",
        "outputId": "04e3785b-b126-4777-c046-bbbc40a06016"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-1408506528>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}